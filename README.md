# ETL Pipeline com Airflow e PySpark

Este projeto implementa um pipeline de ETL (Extract, Transform, Load) utilizando **Apache Airflow**, **PySpark** e Docker. O pipeline processa dados financeiros de receitas e despesas em vÃ¡rias camadas (bronze, silver, gold) e realiza backups automatizados dos resultados.

---

## ğŸš€ Tecnologias Utilizadas

- **Apache Airflow**: OrquestraÃ§Ã£o de tarefas.
- **PySpark**: Processamento de dados.
- **Docker**: ContÃªineres para isolar e gerenciar o ambiente.
- **PostgreSQL** (Opcional): Para armazenamento de dados estruturados.

---

## ğŸ“‚ Estrutura do Projeto

```
projeto_etl/
â”œâ”€â”€ backups/       # Backups automatizados
â”œâ”€â”€ dags/          # Scripts do pipeline ETL
â”œâ”€â”€ data/          # Dados de entrada
â”œâ”€â”€ logs/          # Logs gerados pelo Airflow
â”œâ”€â”€ output/        # Resultados do processamento ETL
â”œâ”€â”€ Dockerfile     # ConfiguraÃ§Ã£o do contÃªiner
â”œâ”€â”€ Airflow.cfg    # ConfiguraÃ§Ã£o refinada do airflow 
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### PrÃ©-requisitos

- **Docker** e **Docker Compose** instalados.
- Conta no GitHub configurada (opcional para entrega).

### Passos de ConfiguraÃ§Ã£o

1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/Sephiroth-jr/Airflow_pyspark.git
   cd Airflow_pyspark
   ```

2. **Inicie os contÃªineres**:
   ```bash
   docker-compose up -d
   ```

3. **Acesse o Airflow**:
   Abra seu navegador em [http://localhost:8080](http://localhost:8080) e use as credenciais padrÃ£o:
   - UsuÃ¡rio: `airflow`
   - Senha: `airflow`

4. **Carregue os dados**:
   Insira os arquivos de entrada (`gdvReceitasExcel.csv` e `gdvDespesasExcel.csv`) no diretÃ³rio `data/`.

5. **Inicie o pipeline**:
   No Airflow, ative e execute a DAG `etl_pipeline`.

---

## ğŸ› ï¸ Componentes do Pipeline

### 1. Camada Bronze
- LÃª os arquivos CSV de entrada.
- Remove colunas desnecessÃ¡rias e realiza limpeza bÃ¡sica dos dados.
- Armazena os dados limpos no formato csv.

### 2. Camada Silver
- Deduplica os dados da camada Bronze.
- Remove valores nulos e ajusta tipos de dados.
- Armazena os dados processados em uma estrutura mais padronizada.

### 3. Camada Gold
- Realiza agregaÃ§Ãµes como totais de receitas e despesas por categoria.
- Produz datasets prontos para anÃ¡lise.

### 4. Backup
- Compacta os resultados do pipeline e cria arquivos de backup no diretÃ³rio `backups/`.

### 5. ValidaÃ§Ã£o
- Verifica a integridade dos arquivos de backup e migra os arquivos para a pasta de validation.

### 6. ExportaÃ§Ã£o para o PostgresSQL
- Verifica os dados da camada gold e faz a importaÃ§Ã£o da tabela de receitas e despesas para o postgres automaticamente apÃ³s execuÃ§Ã£o da dag.

### 7. ConexÃ£o com o Microsoft Power BI via conexÃ£o padrÃ£o PostgresSQL
- Foi feita a conexÃ£o usando o Power BI para criaÃ§Ã£o do dashboard. 

---

1. Certifique-se de que os arquivos gerados pela camada Gold estÃ£o disponÃ­veis em `/opt/airflow/output/gold`.
2. Abra o Power BI e clique em "Obter Dados".
3. Selecione "Arquivos CSV" ou configure a conexÃ£o com o PostgreSQL usando as credenciais:
   - Host: localhost
   - Porta: 5432
   - Banco de Dados: airflow
   - Senha: Airflow
4. Carregue os dados e visualize os dashboards predefinidos.

## RelatÃ³rio Power BI - AnÃ¡lise de Receita e Despesa

![Dashboard de Receita e Despesa](assets/BI - OrÃ§amento.png)

O grÃ¡fico acima mostra a relaÃ§Ã£o entre receita e despesa por categoria de fonte de recurso, destacando os principais numeros extraÃ­dos do relatÃ³rio.

---

## ğŸ” Comandos Ãšteis
### Gerenciamento do Projeto

- **Subir contÃªineres**:
  ```bash
  docker-compose up -d
  ```
- **Parar contÃªineres**:
  ```bash
  docker-compose down
  ```
- **Acessar o Airflow Webserver**:
  [http://localhost:8080](http://localhost:8080)

### Limpar DiretÃ³rios (opcional para reiniciar o projeto)

```bash
rm -rf logs/* output/* backups/*
```

### ConfiguraÃ§Ã£o no Git

```bash
git add .
git commit -m "AtualizaÃ§Ã£o do projeto ETL"
git push origin main
```

---

## ğŸ“ ObservaÃ§Ãµes

- Certifique-se de que os arquivos de entrada (`gdvReceitasExcel.csv` e `gdvDespesasExcel.csv`) estejam na pasta `data/` antes de executar o pipeline.
- Use o `.gitignore` para evitar rastrear pastas como `logs/`, `backups/` e `output/`.

---

## ğŸ“§ Contato

Se tiver dÃºvidas ou sugestÃµes, entre em contato pelo GitHub: [Sephiroth-jr](https://github.com/Sephiroth-jr).
